# -*- coding: utf-8 -*-
"""DistanceFiltered_ML_CatchmentLevel_EmpirDist.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VKF5TGk4NG44FVxmU5qY1-nehdEUbmk4
"""

root = "/home/sethbw/Documents/brian_flow_code/Data/spectralFiles"
outRoot = "/home/sethbw/Documents/brian_flow_code/Data/spectralFiles/figures/ml/catchment"

import math
import pandas as pd
import numpy as np
import seaborn as sns
from scipy import stats
import matplotlib.pyplot as plt
import os

"""#Combining the 40 .csv's"""

file_name_start = 'ml_burnEffects_control_'
files = []
for i in range(40):
    file_name = file_name_start + str(i) + '.csv'
    files.append(os.path.join(root, file_name))

dataframes = []
for f in files:
    df = pd.read_csv(f)
    dataframes.append(df)

comb_df = pd.concat(dataframes, axis=0)
comb_df.to_csv(os.path.join(root, 'ml_burnEffects_control_combined.csv'),index=False)

"""#UNBURNED CATCHMENTS"""

unburned_df = pd.read_csv(os.path.join(root, "ml_burnEffects_control_combined.csv"))

catchments = unburned_df['catchment'].to_numpy()
new_catchs = []
new_catchs_insts = []
# Removing the 'X0' from the start of the catchment string
for catch in catchments:
  underscore_indx = catch.find('_')
  temp_catch = catch[0:underscore_indx]
  temp_catch_insts = catch[10:]
  new_catchs_insts.append(temp_catch_insts)
  new_catchs.append(temp_catch)

unburned_df = unburned_df.loc[:,['numPreYears','postYear','preMean','postMean','postMinusPre','postMinusPrePercent']]
unburned_df['catchment'] = new_catchs
unburned_df['catchment instance'] = new_catchs_insts
unburned_df

master_burn_data_path = os.path.join(root, 'master_burn_data.csv')
master_burn_data_df = pd.read_csv(master_burn_data_path)

# catchments = master_burn_data_df['catchment'].to_numpy()
# new_catchs = []
# # Removing the 'X' from the start of the catchment string
# for catch in catchments:
#   temp_catch = catch[1:15]
#   new_catchs.append(temp_catch)

master_burn_data_df = master_burn_data_df.loc[:,['catchment','burned','distance_to_nearest_burn']]
master_burn_data_df

unburned_df = unburned_df.set_index('catchment').join(master_burn_data_df.set_index('catchment'))
unburned_df
unburned_df_lt200 = unburned_df[unburned_df.distance_to_nearest_burn < 200]
unburned_df_lt100 = unburned_df[unburned_df.distance_to_nearest_burn < 100]

unburned_df_lt200

unburned_df_lt100

unburned_comb_df = unburned_df_lt200.loc[:,['postMinusPrePercent','postYear','catchment instance','distance_to_nearest_burn']]
unburned_post_yr_1 = unburned_comb_df.loc[unburned_comb_df.postYear == 1]
unburned_post_yr_2 = unburned_comb_df.loc[unburned_comb_df.postYear == 2]
unburned_post_yr_3 = unburned_comb_df.loc[unburned_comb_df.postYear == 3]
unburned_post_yr_4 = unburned_comb_df.loc[unburned_comb_df.postYear == 4]

"""#BURNED CATCHMENTS"""

burned_df = pd.read_csv(os.path.join(root, 'ml_burnEffects_real.csv'))
burned_df

burned_comb_df = burned_df.loc[:,['postMinusPrePercent','postYear']]
burned_post_yr_1 = burned_comb_df.loc[burned_comb_df.postYear == 1]
burned_post_yr_2 = burned_comb_df.loc[burned_comb_df.postYear == 2]
burned_post_yr_3 = burned_comb_df.loc[burned_comb_df.postYear == 3]
burned_post_yr_4 = burned_comb_df.loc[burned_comb_df.postYear == 4]

# catchments = burned_df['catchment'].to_numpy()
# new_catchs = []
# # Removing the 'X0' from the start of the catchment string
# for catch in catchments:
#   temp_catch = int(catch[2:9])
#   new_catchs.append(temp_catch)

burned_df = burned_df.loc[:,['catchment','numPreYears','postYear','preMean','postMean','postMinusPre','postMinusPrePercent']]
burned_df

"""get percent burn data"""

catchmentsForQAnalysis_path = "/home/sethbw/Documents/brian_flow_code/Data/metadata/catchments_for_Q_analysis.csv"
CQA_df = pd.read_csv(catchmentsForQAnalysis_path)

CQA_df = CQA_df.loc[:,['GAGE_ID','year','percent_burned']]
CQA_df['GAGE_ID'] = CQA_df['GAGE_ID'].values.astype(int).tolist() # cast to int because it was previously a float

new_cats = []
for cat in CQA_df['GAGE_ID']:
  cat_str = str(cat)
  if len(cat_str) < 7:
    print('BAD')
  if len(cat_str) == 7:
    cat_str = "0" + cat_str
  cat_str = "X" + cat_str
  new_cats.append(cat_str)

CQA_df['catchment'] = new_cats
CQA_df = CQA_df.drop(['GAGE_ID'], axis=1)

CQA_df

ML_CQA_joined_df = burned_df.set_index('catchment').join(CQA_df.set_index('catchment'))
ML_CQA_joined_df = ML_CQA_joined_df[ML_CQA_joined_df.percent_burned.notna()]
ML_CQA_joined_df

ML_CQA_yr_1 = ML_CQA_joined_df.loc[ML_CQA_joined_df.postYear == 1]
ML_CQA_yr_2 = ML_CQA_joined_df.loc[ML_CQA_joined_df.postYear == 2]
ML_CQA_yr_3 = ML_CQA_joined_df.loc[ML_CQA_joined_df.postYear == 3]
ML_CQA_yr_4 = ML_CQA_joined_df.loc[ML_CQA_joined_df.postYear == 4]

ML_CQA_yr_1_gr10per = ML_CQA_yr_1.loc[ML_CQA_yr_1.percent_burned > .1]
ML_CQA_yr_1_gr10per_arr = ML_CQA_yr_1_gr10per['postMinusPrePercent'].to_numpy()

ML_CQA_yr_2_gr10per = ML_CQA_yr_2.loc[ML_CQA_yr_2.percent_burned > .1]
ML_CQA_yr_2_gr10per_arr = ML_CQA_yr_2_gr10per['postMinusPrePercent'].to_numpy()

ML_CQA_yr_3_gr10per = ML_CQA_yr_3.loc[ML_CQA_yr_3.percent_burned > .1]
ML_CQA_yr_3_gr10per_arr = ML_CQA_yr_3_gr10per['postMinusPrePercent'].to_numpy()

ML_CQA_yr_4_gr10per = ML_CQA_yr_4.loc[ML_CQA_yr_4.percent_burned > .1]
ML_CQA_yr_4_gr10per_arr = ML_CQA_yr_4_gr10per['postMinusPrePercent'].to_numpy()


def confidence_interval(df, confidence=0.9):
  df_size = len(df)
  df_conf = round(df_size*confidence)
  sorted_df = df.sort_values(["data"], ascending=False,ignore_index=True)
  tails = df_size - df_conf
  tail_1 = None
  tail_2 = None
  if tails%2 == 1:
    tail_1 = -(tails // -2) # ceiling division
    tail_2 = tails // 2 # floor division
  else:
    tail_1 = tails / 2
    tail_2 = tails / 2
  tail_2 = df_size - tail_2
  
  tail_1 = int(tail_1)
  tail_2 = int(tail_2)
  left_bound = sorted_df['data'].iloc[tail_1]
  right_bound = sorted_df['data'].iloc[tail_2]
  return (right_bound, left_bound)

"""###Year 1"""

unburned_yr1_list = unburned_post_yr_1['postMinusPrePercent'].to_numpy()
unburned_yr1_arr = np.array(unburned_yr1_list)
unburned_z_scores = stats.zscore(unburned_yr1_arr,ddof=0)
d = {'data':unburned_yr1_arr, 'z_score':unburned_z_scores}
unburned_yr1_df = pd.DataFrame(d)
unburned_yr1_df['outlier'] = (abs(unburned_yr1_df['z_score'])>3).astype(int)
unburned_no_outliers_df = unburned_yr1_df[unburned_yr1_df.outlier == 0]
unburned_yr1_no_outliers_arr = unburned_no_outliers_df['data'].to_numpy()
unburned_yr1_no_outliers_mean = np.mean(unburned_yr1_no_outliers_arr)
unburned_yr1_with_outliers_mean = np.mean(unburned_yr1_arr)
ub_yr1_confidence_int_90 = confidence_interval(unburned_yr1_df,confidence=.9)
ub_yr1_confidence_int_95 = confidence_interval(unburned_yr1_df,confidence=.95)
ub_yr1_confidence_int_97_5 = confidence_interval(unburned_yr1_df,confidence=.975)
print('90% confidence interval: ', ub_yr1_confidence_int_90)
print('95% confidence interval: ', ub_yr1_confidence_int_95)
print('97.5% confidence interval: ', ub_yr1_confidence_int_97_5)
print('unburned_yr1_no_outliers_mean: ', unburned_yr1_no_outliers_mean)
print('unburned_yr1_with_outliers_mean: ', np.mean(unburned_yr1_arr))

burned_yr1_list = burned_post_yr_1['postMinusPrePercent'].to_numpy()
burned_yr1_arr = np.array(burned_yr1_list)
burned_z_scores = stats.zscore(burned_yr1_arr,ddof=0)
d = {'data':burned_yr1_arr, 'z_score':burned_z_scores}
burned_yr1_df = pd.DataFrame(d)
burned_yr1_df['outlier'] = (abs(burned_yr1_df['z_score'])>3).astype(int)
burned_no_outliers_df = burned_yr1_df[burned_yr1_df.outlier == 0]
burned_yr1_no_outliers_arr = burned_no_outliers_df['data'].to_numpy()
burned_yr1_no_outliers_mean = np.mean(burned_yr1_no_outliers_arr)
burned_yr1_with_outliers_mean = np.mean(burned_yr1_arr)
b_yr1_confidence_int_90 = confidence_interval(burned_yr1_df,confidence=.9)
b_yr1_confidence_int_95 = confidence_interval(burned_yr1_df,confidence=.95)
b_yr1_confidence_int_97_5 = confidence_interval(burned_yr1_df,confidence=.975)
print('90% confidence interval: ', b_yr1_confidence_int_90)
print('95% confidence interval: ', b_yr1_confidence_int_95)
print('97.5% confidence interval: ', b_yr1_confidence_int_97_5)
print('burned_yr1_no_outliers_mean: ', burned_yr1_no_outliers_mean)
print('burned_yr1_with_outliers_mean: ', np.mean(burned_yr1_arr))


"""###Year 2"""

unburned_yr2_list = unburned_post_yr_2['postMinusPrePercent'].to_numpy()
unburned_yr2_arr = np.array(unburned_yr2_list)
unburned_z_scores = stats.zscore(unburned_yr2_arr,ddof=0)
d = {'data':unburned_yr2_arr, 'z_score':unburned_z_scores}
unburned_yr2_df = pd.DataFrame(d)
unburned_yr2_df['outlier'] = (abs(unburned_yr2_df['z_score'])>3).astype(int)
unburned_no_outliers_df = unburned_yr2_df[unburned_yr2_df.outlier == 0]
unburned_yr2_no_outliers_arr = unburned_no_outliers_df['data'].to_numpy()
unburned_yr2_no_outliers_mean = np.mean(unburned_yr2_no_outliers_arr)
unburned_yr2_with_outliers_mean = np.mean(unburned_yr2_arr)
ub_yr2_confidence_int_90 = confidence_interval(unburned_yr2_df,confidence=.9)
ub_yr2_confidence_int_95 = confidence_interval(unburned_yr2_df,confidence=.95)
ub_yr2_confidence_int_97_5 = confidence_interval(unburned_yr2_df,confidence=.975)
print('90% confidence interval: ', ub_yr2_confidence_int_90)
print('95% confidence interval: ', ub_yr2_confidence_int_95)
print('97.5% confidence interval: ', ub_yr2_confidence_int_97_5)
print('unburned_yr2_no_outliers_mean: ', unburned_yr2_no_outliers_mean)
print('unburned_yr2_with_outliers_mean: ', np.mean(unburned_yr2_arr))

burned_yr2_list = burned_post_yr_2['postMinusPrePercent'].to_numpy()
burned_yr2_arr = np.array(burned_yr2_list)
burned_z_scores = stats.zscore(burned_yr2_arr,ddof=0)
d = {'data':burned_yr2_arr, 'z_score':burned_z_scores}
burned_yr2_df = pd.DataFrame(d)
burned_yr2_df['outlier'] = (abs(burned_yr2_df['z_score'])>3).astype(int)
burned_no_outliers_df = burned_yr2_df[burned_yr2_df.outlier == 0]
burned_yr2_no_outliers_arr = burned_no_outliers_df['data'].to_numpy()
burned_yr2_no_outliers_mean = np.mean(burned_yr2_no_outliers_arr)
burned_yr2_with_outliers_mean = np.mean(burned_yr2_arr)
b_yr2_confidence_int_90 = confidence_interval(burned_yr2_df,confidence=.9)
b_yr2_confidence_int_95 = confidence_interval(burned_yr2_df,confidence=.95)
b_yr2_confidence_int_97_5 = confidence_interval(burned_yr2_df,confidence=.975)
print('90% confidence interval: ', b_yr2_confidence_int_90)
print('95% confidence interval: ', b_yr2_confidence_int_95)
print('97.5% confidence interval: ', b_yr2_confidence_int_97_5)
print('burned_yr2_no_outliers_mean: ', burned_yr2_no_outliers_mean)
print('burned_yr2_with_outliers_mean: ', np.mean(burned_yr2_arr))

"""###Year 3"""

unburned_yr3_list = unburned_post_yr_3['postMinusPrePercent'].to_numpy()
unburned_yr3_arr = np.array(unburned_yr3_list)
unburned_z_scores = stats.zscore(unburned_yr3_arr,ddof=0)
d = {'data':unburned_yr3_arr, 'z_score':unburned_z_scores}
unburned_yr3_df = pd.DataFrame(d)
unburned_yr3_df['outlier'] = (abs(unburned_yr3_df['z_score'])>3).astype(int)
unburned_no_outliers_df = unburned_yr3_df[unburned_yr3_df.outlier == 0]
unburned_yr3_no_outliers_arr = unburned_no_outliers_df['data'].to_numpy()
unburned_yr3_no_outliers_mean = np.mean(unburned_yr3_no_outliers_arr)
unburned_yr3_with_outliers_mean = np.mean(unburned_yr3_arr)
ub_yr3_confidence_int_90 = confidence_interval(unburned_yr3_df,confidence=.9)
ub_yr3_confidence_int_95 = confidence_interval(unburned_yr3_df,confidence=.95)
ub_yr3_confidence_int_97_5 = confidence_interval(unburned_yr3_df,confidence=.975)
print('90% confidence interval: ', ub_yr3_confidence_int_90)
print('95% confidence interval: ', ub_yr3_confidence_int_95)
print('97.5% confidence interval: ', ub_yr3_confidence_int_97_5)
print('unburned_yr3_no_outliers_mean: ', unburned_yr3_no_outliers_mean)
print('unburned_yr3_with_outliers_mean: ', np.mean(unburned_yr3_arr))

burned_yr3_list = burned_post_yr_3['postMinusPrePercent'].to_numpy()
burned_yr3_arr = np.array(burned_yr3_list)
burned_z_scores = stats.zscore(burned_yr3_arr,ddof=0)
d = {'data':burned_yr3_arr, 'z_score':burned_z_scores}
burned_yr3_df = pd.DataFrame(d)
burned_yr3_df['outlier'] = (abs(burned_yr3_df['z_score'])>3).astype(int)
burned_no_outliers_df = burned_yr3_df[burned_yr3_df.outlier == 0]
burned_yr3_no_outliers_arr = burned_no_outliers_df['data'].to_numpy()
burned_yr3_no_outliers_mean = np.mean(burned_yr3_no_outliers_arr)
burned_yr3_with_outliers_mean = np.mean(burned_yr3_arr)
b_yr3_confidence_int_90 = confidence_interval(burned_yr3_df,confidence=.9)
b_yr3_confidence_int_95 = confidence_interval(burned_yr3_df,confidence=.95)
b_yr3_confidence_int_97_5 = confidence_interval(burned_yr3_df,confidence=.975)
print('90% confidence interval: ', b_yr3_confidence_int_90)
print('95% confidence interval: ', b_yr3_confidence_int_95)
print('97.5% confidence interval: ', b_yr3_confidence_int_97_5)
print('burned_yr3_no_outliers_mean: ', burned_yr3_no_outliers_mean)
print('burned_yr3_with_outliers_mean: ', np.mean(burned_yr3_arr))

"""###Year 4"""

unburned_yr4_list = unburned_post_yr_4['postMinusPrePercent'].to_numpy()
unburned_yr4_arr = np.array(unburned_yr4_list)
unburned_z_scores = stats.zscore(unburned_yr4_arr,ddof=0)
d = {'data':unburned_yr4_arr, 'z_score':unburned_z_scores}
unburned_yr4_df = pd.DataFrame(d)
unburned_yr4_df['outlier'] = (abs(unburned_yr4_df['z_score'])>4).astype(int)
unburned_no_outliers_df = unburned_yr4_df[unburned_yr4_df.outlier == 0]
unburned_yr4_no_outliers_arr = unburned_no_outliers_df['data'].to_numpy()
unburned_yr4_no_outliers_mean = np.mean(unburned_yr4_no_outliers_arr)
unburned_yr4_with_outliers_mean = np.mean(unburned_yr4_arr)
ub_yr4_confidence_int_90 = confidence_interval(unburned_yr4_df,confidence=.9)
ub_yr4_confidence_int_95 = confidence_interval(unburned_yr4_df,confidence=.95)
ub_yr4_confidence_int_97_5 = confidence_interval(unburned_yr4_df,confidence=.975)
print('90% confidence interval: ', ub_yr4_confidence_int_90)
print('95% confidence interval: ', ub_yr4_confidence_int_95)
print('97.5% confidence interval: ', ub_yr4_confidence_int_97_5)
print('unburned_yr4_no_outliers_mean: ', unburned_yr4_no_outliers_mean)
print('unburned_yr4_with_outliers_mean: ', np.mean(unburned_yr4_arr))

burned_yr4_list = burned_post_yr_4['postMinusPrePercent'].to_numpy()
burned_yr4_arr = np.array(burned_yr4_list)
burned_z_scores = stats.zscore(burned_yr4_arr,ddof=0)
d = {'data':burned_yr4_arr, 'z_score':burned_z_scores}
burned_yr4_df = pd.DataFrame(d)
burned_yr4_df['outlier'] = (abs(burned_yr4_df['z_score'])>4).astype(int)
burned_no_outliers_df = burned_yr4_df[burned_yr4_df.outlier == 0]
burned_yr4_no_outliers_arr = burned_no_outliers_df['data'].to_numpy()
burned_yr4_no_outliers_mean = np.mean(burned_yr4_no_outliers_arr)
burned_yr4_with_outliers_mean = np.mean(burned_yr4_arr)
b_yr4_confidence_int_90 = confidence_interval(burned_yr4_df,confidence=.9)
b_yr4_confidence_int_95 = confidence_interval(burned_yr4_df,confidence=.95)
b_yr4_confidence_int_97_5 = confidence_interval(burned_yr4_df,confidence=.975)
print('90% confidence interval: ', b_yr4_confidence_int_90)
print('95% confidence interval: ', b_yr4_confidence_int_95)
print('97.5% confidence interval: ', b_yr4_confidence_int_97_5)
print('burned_yr4_no_outliers_mean: ', burned_yr4_no_outliers_mean)
print('burned_yr4_with_outliers_mean: ', np.mean(burned_yr4_arr))

"""#Histogram of PostMinusPrePercent"""

unburned_bins = list(range(-2000,1000,50))
unburned_bins = np.array(unburned_bins)
unburned_bins = unburned_bins 
print(unburned_bins)

fig, ax1 = plt.subplots()
ax1.set_xlabel('Percent Change')
ax1.set_ylabel('Burned Catchment Count')
ax1.hist(burned_yr1_arr*100, bins=unburned_bins, stacked=True, color='orange',alpha=0.6, label='Burned Catchments')

ax2 = ax1.twinx()
ax2.set_ylabel('Unburned Catchment Count')
ax2.hist(unburned_yr1_arr*100, bins=unburned_bins, stacked=True, color='blue',alpha=0.5, label='Unburned Catchments')



plt.title('1 Year After Burn')
fig.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)
#plt.tight_layout()
plt.savefig(os.path.join(outRoot, "ml_all_yr1.png"))
plt.show()

fig, ax1 = plt.subplots()
ax1.set_xlabel('Percent Change')
ax1.set_ylabel('Burned Catchment Count')
ax1.hist(ML_CQA_yr_1_gr10per_arr*100, bins=unburned_bins, stacked=True, color='red',label='> 10% Burned Catchments')

ax2 = ax1.twinx()
ax2.set_ylabel('Unburned Catchment Count')
ax2.hist(unburned_yr1_arr*100, bins=unburned_bins, stacked=True, color='blue',alpha=0.6, label='Unburned Catchments')

plt.title('1 Year After Burn')
fig.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)
#plt.tight_layout()
plt.savefig(os.path.join(outRoot, "ml_g10_yr1.png"))
plt.show()

fig, ax1 = plt.subplots()
ax1.set_xlabel('Percent Change')
ax1.set_ylabel('Burned Catchment Count')
ax1.hist(burned_yr2_arr*100, bins=unburned_bins, stacked=True, color='orange',label='Burned Catchments')

ax2 = ax1.twinx()
ax2.set_ylabel('Unburned Catchment Count')
ax2.hist(unburned_yr2_arr*100, bins=unburned_bins, stacked=True, color='blue',alpha=0.6, label='Unburned Catchments')

plt.title('2 Year After Burn')
fig.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)
#plt.tight_layout()
plt.savefig(os.path.join(outRoot, "ml_all_yr2.png"))
plt.show()

fig, ax1 = plt.subplots()
ax1.set_xlabel('Percent Change')
ax1.set_ylabel('Burned Catchment Count')
ax1.hist(ML_CQA_yr_2_gr10per_arr*100, bins=unburned_bins, stacked=True, color='red',label='> 10% Burned Catchments')

ax2 = ax1.twinx()
ax2.set_ylabel('Unburned Catchment Count')
ax2.hist(unburned_yr2_arr*100, bins=unburned_bins, stacked=True, color='blue',alpha=0.6, label='Unburned Catchments')

plt.title('2 Year After Burn')
fig.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)
#plt.tight_layout()
plt.savefig(os.path.join(outRoot, "ml_g10_yr2.png"))
plt.show()

fig, ax1 = plt.subplots()
ax1.set_xlabel('Percent Change')
ax1.set_ylabel('Burned Catchment Count')
ax1.hist(burned_yr3_arr*100, bins=unburned_bins, stacked=True, color='orange',label='Burned Catchments')

ax2 = ax1.twinx()
ax2.set_ylabel('Unburned Catchment Count')
ax2.hist(unburned_yr3_arr*100, bins=unburned_bins, stacked=True, color='blue',alpha=0.6, label='Unburned Catchments')


plt.title('3 Year After Burn')
fig.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)
#plt.tight_layout()
plt.savefig(os.path.join(outRoot, "ml_all_yr3.png"))
plt.show()

fig, ax1 = plt.subplots()
ax1.set_xlabel('Percent Change')
ax1.set_ylabel('Burned Catchment Count')
ax1.hist(ML_CQA_yr_3_gr10per_arr*100, bins=unburned_bins, stacked=True, color='red',label='> 10% Burned Catchments')

ax2 = ax1.twinx()
ax2.set_ylabel('Unburned Catchment Count')
ax2.hist(unburned_yr3_arr*100, bins=unburned_bins, stacked=True, color='blue',alpha=0.6, label='Unburned Catchments')


plt.title('3 Year After Burn')
fig.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)
#plt.tight_layout()
plt.savefig(os.path.join(outRoot, "ml_g10_yr3.png"))
plt.show()

fig, ax1 = plt.subplots()
ax1.set_xlabel('Percent Change')
ax1.set_ylabel('Burned Catchment Count')
ax1.hist(burned_yr4_arr*100, bins=unburned_bins, stacked=True, color='orange',label='Burned Catchments')

ax2 = ax1.twinx()
ax2.set_ylabel('Unburned Catchment Count')
ax2.hist(unburned_yr4_arr*100, bins=unburned_bins, stacked=True, color='blue',alpha=0.6, label='Unburned Catchments')


plt.title('4 Year After Burn')
fig.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)
#plt.tight_layout()
plt.savefig(os.path.join(outRoot, "ml_all_yr4.png"))
plt.show()

fig, ax1 = plt.subplots()
ax1.set_xlabel('Percent Change')
ax1.set_ylabel('Burned Catchment Count')
ax1.hist(ML_CQA_yr_4_gr10per_arr*100, bins=unburned_bins, stacked=True, color='red',label='> 10% Burned Catchments')

ax2 = ax1.twinx()
ax2.set_ylabel('Unburned Catchment Count')
ax2.hist(unburned_yr4_arr*100, bins=unburned_bins, stacked=True, color='blue',alpha=0.6, label='Unburned Catchments')

plt.title('4 Year After Burn')
fig.legend(bbox_to_anchor=(0, 1), loc='upper left', ncol=1)
#plt.tight_layout()
plt.savefig(os.path.join(outRoot, "ml_g10_yr4.png"))
plt.show()

"""# Scatter plots for burned basins"""

plt.scatter(ML_CQA_yr_1['percent_burned'].to_numpy(), ML_CQA_yr_1['postMinusPrePercent'].to_numpy())
plt.xlabel('percent burned')
plt.ylabel('measured effect proportion')

plt.scatter(ML_CQA_yr_2['percent_burned'].to_numpy(), ML_CQA_yr_2['postMinusPrePercent'].to_numpy())
plt.xlabel('percent burned')
plt.ylabel('measured effect proportion')

plt.scatter(ML_CQA_yr_3['percent_burned'].to_numpy(), ML_CQA_yr_3['postMinusPrePercent'].to_numpy())
plt.xlabel('percent burned')
plt.ylabel('measured effect proportion')

plt.scatter(ML_CQA_yr_4['percent_burned'].to_numpy(), ML_CQA_yr_4['postMinusPrePercent'].to_numpy())
plt.xlabel('percent burned')
plt.ylabel('measured effect proportion')

"""#Statistics Table"""

# unburned_means_no_outliers = [unburned_yr1_no_outliers_mean, unburned_yr2_no_outliers_mean, unburned_yr3_no_outliers_mean, unburned_yr4_no_outliers_mean]
# unburned_means_yes_outliers = [unburned_yr1_with_outliers_mean, unburned_yr2_with_outliers_mean, unburned_yr3_with_outliers_mean, unburned_yr4_with_outliers_mean]
# unburned_90percent_confidence = [ub_yr1_confidence_int_90, ub_yr2_confidence_int_90, ub_yr3_confidence_int_90, ub_yr4_confidence_int_90]
# unburned_95percent_confidence = [ub_yr1_confidence_int_95, ub_yr2_confidence_int_95, ub_yr3_confidence_int_95, ub_yr4_confidence_int_95]
# unburned_97_5_percent_confidence = [ub_yr1_confidence_int_97_5, ub_yr2_confidence_int_97_5, ub_yr3_confidence_int_97_5, ub_yr4_confidence_int_97_5]
# unburned_d = {'post year':[1,2,3,4], 
#               'means no outliers':unburned_means_no_outliers, 
#               'means with outliers': unburned_means_yes_outliers, 
#               '90 percent confidence interval': unburned_90percent_confidence, 
#               '95 percent confidence interval': unburned_95percent_confidence, 
#               '97.5 percent confidence interval': unburned_95percent_confidence}
# unburned_df = pd.DataFrame(unburned_d)
# unburned_df.to_csv(path_or_buf='/content/drive/MyDrive/IDeA Lab/Burned Catchment Hydrology/ML/outputs/ml_catch-level_empDist_unburned_table.csv',index=False)

# burned_means_no_outliers = [burned_yr1_no_outliers_mean, burned_yr2_no_outliers_mean, burned_yr3_no_outliers_mean, burned_yr4_no_outliers_mean]
# burned_means_yes_outliers = [burned_yr1_with_outliers_mean, burned_yr2_with_outliers_mean, burned_yr3_with_outliers_mean, burned_yr4_with_outliers_mean]
# burned_90percent_confidence = [b_yr1_confidence_int_90, b_yr2_confidence_int_90, b_yr3_confidence_int_90, b_yr4_confidence_int_90]
# burned_95percent_confidence = [b_yr1_confidence_int_95, b_yr2_confidence_int_95, b_yr3_confidence_int_95, b_yr4_confidence_int_95]
# burned_97_5_percent_confidence = [b_yr1_confidence_int_97_5, b_yr2_confidence_int_97_5, b_yr3_confidence_int_97_5, b_yr4_confidence_int_97_5]
# burned_d = {'post year':[1,2,3,4], 
#               'means no outliers':burned_means_no_outliers, 
#               'means with outliers': burned_means_yes_outliers, 
#               '90 percent confidence interval': burned_90percent_confidence, 
#               '95 percent confidence interval': burned_95percent_confidence, 
#               '97.5 percent confidence interval': burned_95percent_confidence}
# burned_df = pd.DataFrame(burned_d)
# burned_df.to_csv(path_or_buf='/content/drive/MyDrive/IDeA Lab/Burned Catchment Hydrology/ML/outputs/ml_catch-level_empDist_burned_table.csv',index=False)
